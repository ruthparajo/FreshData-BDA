{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 61.0M  100 61.0M    0     0  12.8M      0  0:00:04  0:00:04 --:--:-- 12.8M\n"
     ]
    }
   ],
   "source": [
    "#!curl -o duckdb.jar \"https://repo1.maven.org/maven2/org/duckdb/duckdb_jdbc/0.10.1/duckdb_jdbc-0.10.1.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/13 23:56:43 WARN Utils: Your hostname, Ruths-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 192.168.28.41 instead (on interface en0)\n",
      "24/04/13 23:56:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/04/13 23:56:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkSession.builder\\\n",
    "    .config(\"spark.jars\", \"duckdb.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['formatted_zone/compravenda_sup.db', 'formatted_zone/rent_price.db', 'formatted_zone/renda.db']\n",
    "col_numeriques = {'formatted_zone/compravenda_sup.db': ['Nombre']\n",
    "                  , 'formatted_zone/renda.db': ['Import_Euros']\n",
    "                , 'formatted_zone/rent_price.db': ['Price']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mirar coses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------------+--------------+----------+--------------------+----------------------------------+------+\n",
      "| Any|Trimestre|Codi_Districte| Nom_Districte|Codi_Barri|           Nom_Barri|Superfície_mitjana_(m2_construïts)|Nombre|\n",
      "+----+---------+--------------+--------------+----------+--------------------+----------------------------------+------+\n",
      "|2023|        1|             1|  Ciutat Vella|         1|            el Raval|                             Total|  63.8|\n",
      "|2023|        1|             1|  Ciutat Vella|         2|      el Barri Gòtic|                             Total|  82.9|\n",
      "|2023|        1|             1|  Ciutat Vella|         3|      la Barceloneta|                             Total|  41.3|\n",
      "|2023|        1|             1|  Ciutat Vella|         4|Sant Pere, Santa ...|                             Total|  70.9|\n",
      "|2023|        1|             2|      Eixample|         5|       el Fort Pienc|                             Total|  80.6|\n",
      "|2023|        1|             2|      Eixample|         6|  la Sagrada Família|                             Total|  72.4|\n",
      "|2023|        1|             2|      Eixample|         7|la Dreta de l'Eix...|                             Total| 116.5|\n",
      "|2023|        1|             2|      Eixample|         8|l'Antiga Esquerra...|                             Total|  99.5|\n",
      "|2023|        1|             2|      Eixample|         9|la Nova Esquerra ...|                             Total|  74.7|\n",
      "|2023|        1|             2|      Eixample|        10|         Sant Antoni|                             Total|  81.3|\n",
      "|2023|        1|             3|Sants-Montjuïc|        11|        el Poble Sec|                             Total|  61.6|\n",
      "|2023|        1|             3|Sants-Montjuïc|        12|la Marina del Pra...|                             Total|  78.7|\n",
      "|2023|        1|             3|Sants-Montjuïc|        13|   la Marina de Port|                             Total|  69.5|\n",
      "|2023|        1|             3|Sants-Montjuïc|        14|la Font de la Gua...|                             Total|  78.5|\n",
      "|2023|        1|             3|Sants-Montjuïc|        15|         Hostafrancs|                             Total|  67.5|\n",
      "|2023|        1|             3|Sants-Montjuïc|        16|          la Bordeta|                             Total|  78.7|\n",
      "|2023|        1|             3|Sants-Montjuïc|        17|       Sants - Badal|                             Total|  62.9|\n",
      "|2023|        1|             3|Sants-Montjuïc|        18|               Sants|                             Total|  73.2|\n",
      "|2023|        1|             4|     Les Corts|        19|           les Corts|                             Total|  80.9|\n",
      "|2023|        1|             4|     Les Corts|        20|la Maternitat i S...|                             Total|  91.0|\n",
      "+----+---------+--------------+--------------+----------+--------------------+----------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Row(Superfície_mitjana_(m2_construïts)='Total')\n",
      "Row(Superfície_mitjana_(m2_construïts)='Habitatge nou protegit')\n",
      "Row(Superfície_mitjana_(m2_construïts)='Habitatge usat')\n",
      "Row(Superfície_mitjana_(m2_construïts)='Habitatge nou lliure')\n",
      "+----+---------+--------------+--------------------+--------------------+------+\n",
      "|Year|Trimester|      District|       Neighbourhood|       Average _rent| Price|\n",
      "+----+---------+--------------+--------------------+--------------------+------+\n",
      "|2014|        1|  Ciutat Vella|            el Raval|average rent (eur...|589.55|\n",
      "|2014|        1|  Ciutat Vella|      Gothic Quarter|average rent (eur...|712.79|\n",
      "|2014|        1|  Ciutat Vella|      la Barceloneta|average rent (eur...|540.71|\n",
      "|2014|        1|  Ciutat Vella|Sant Pere, Santa ...|average rent (eur...|673.44|\n",
      "|2014|        1|      Eixample|          Fort Pienc|average rent (eur...|736.09|\n",
      "|2014|        1|      Eixample|     Sagrada Familia|average rent (eur...|673.37|\n",
      "|2014|        1|      Eixample|la Dreta de l'Eix...|average rent (eur...| 921.4|\n",
      "|2014|        1|      Eixample|l'Antiga Esquerra...|average rent (eur...|827.87|\n",
      "|2014|        1|      Eixample|la Nova Esquerra ...|average rent (eur...|716.13|\n",
      "|2014|        1|      Eixample|         Sant Antoni|average rent (eur...|693.43|\n",
      "|2014|        1|Sants-Montjuic|        el Poble Sec|average rent (eur...| 568.0|\n",
      "|2014|        1|Sants-Montjuic|   la Marina de Port|average rent (eur...|553.55|\n",
      "|2014|        1|Sants-Montjuic|la Font de la Gua...|average rent (eur...| 631.5|\n",
      "|2014|        1|Sants-Montjuic|         Hostafrancs|average rent (eur...|580.71|\n",
      "|2014|        1|Sants-Montjuic|          la Bordeta|average rent (eur...|604.74|\n",
      "|2014|        1|Sants-Montjuic|       Sants - Badal|average rent (eur...|584.27|\n",
      "|2014|        1|Sants-Montjuic|               Sants|average rent (eur...|605.28|\n",
      "|2014|        1|     Les Corts|           les Corts|average rent (eur...|777.23|\n",
      "|2014|        1|     Les Corts|la Maternitat i S...|average rent (eur...|700.16|\n",
      "|2014|        1|     Les Corts|           Pedralbes|average rent (eur...|1230.0|\n",
      "+----+---------+--------------+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Row(Average _rent='average rent (euro/month)')\n",
      "Row(Average _rent='average rent per surface (euro/m2)')\n",
      "+----+--------------+-------------+----------+---------+-------------+------------+\n",
      "| Any|Codi_Districte|Nom_Districte|Codi_Barri|Nom_Barri|Seccio_Censal|Import_Euros|\n",
      "+----+--------------+-------------+----------+---------+-------------+------------+\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|            1|       28484|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|            2|       23611|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|            3|       26607|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|            4|       28688|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|            5|       23769|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|            6|       27156|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|            7|       27564|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|            8|       22995|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|            9|       25455|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           10|       25264|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           11|       22456|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           12|       27229|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           13|       30275|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           14|       28056|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           15|       27607|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           16|       26940|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           17|       26609|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           18|       27748|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           19|       25122|\n",
      "|2021|             1| Ciutat Vella|         1| el Raval|           20|       35095|\n",
      "+----+--------------+-------------+----------+---------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "\n",
    "    #llegim db\n",
    "    RDD=sc.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:duckdb:{path}\") \\\n",
    "        .option(\"dbtable\", \"RDD\") \\\n",
    "        .option(\"driver\", \"org.duckdb.DuckDBDriver\").load()\n",
    "    RDD.show()\n",
    "    if \"Superfície_mitjana_(m2_construïts)\" in RDD.columns:\n",
    "        valores_unicos = RDD.select('Superfície_mitjana_(m2_construïts)').distinct().collect()\n",
    "        for valor in valores_unicos:\n",
    "            print(valor)\n",
    "    elif \"Average _rent\" in RDD.columns:\n",
    "        valores_unicos = RDD.select('Average _rent').distinct().collect()\n",
    "        for valor in valores_unicos:\n",
    "            print(valor)\n",
    "    \n",
    "    #per guardar:\n",
    "    '''RDD.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:duckdb:{path}\") \\\n",
    "        .option(\"dbtable\", \"RDD\") \\\n",
    "        .option(\"driver\", \"org.duckdb.DuckDBDriver\") \\\n",
    "        .save()'''\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prova codi esteban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "  warnings.warn(\"Please install psutil to have better \" \"support with spilling\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'decimal.Decimal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruthparajo/Desktop/uni/BDA/projecte final/FreshData-BDA/trusted_zone.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m valor_min \u001b[39m=\u001b[39m RDD\u001b[39m.\u001b[39mrdd\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m f: \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(f[columna]))\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: (\u001b[39m1\u001b[39m, x[columna]))\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m.\u001b[39mmapValues(\u001b[39mlambda\u001b[39;00m x: \u001b[39mfloat\u001b[39m(x) \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(x) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m \u001b[39melse\u001b[39;00m x)\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mreduceByKey(\u001b[39mlambda\u001b[39;00m x, y: \u001b[39mmin\u001b[39m(x,y))\u001b[39m.\u001b[39mcollect()[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m valor_max \u001b[39m=\u001b[39m RDD\u001b[39m.\u001b[39mrdd\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m f: \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(f[columna]))\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: (\u001b[39m1\u001b[39m, \u001b[39mfloat\u001b[39m(x[columna])))\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39mreduceByKey(\u001b[39mlambda\u001b[39;00m x, y: \u001b[39mmax\u001b[39m(x,y))\u001b[39m.\u001b[39mcollect()[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m RDD \u001b[39m=\u001b[39m RDD\u001b[39m.\u001b[39mwithColumn(columna, (RDD[columna] \u001b[39m-\u001b[39m valor_min)\u001b[39m/\u001b[39m (valor_max \u001b[39m-\u001b[39;49m valor_min))\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'decimal.Decimal'"
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    RDD=sc.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:duckdb:{path}\") \\\n",
    "        .option(\"dbtable\", \"RDD\") \\\n",
    "        .option(\"driver\", \"org.duckdb.DuckDBDriver\").load()\n",
    "   \n",
    "    for columna in col_numeriques[path]:\n",
    "            valor_min = RDD.rdd.filter(lambda f: '-' not in str(f[columna]))\\\n",
    "                .map(lambda x: (1, x[columna]))\\\n",
    "                .mapValues(lambda x: float(x) if type(x) == str else x)\\\n",
    "                .reduceByKey(lambda x, y: min(x,y)).collect()[0][1]\n",
    "            valor_max = RDD.rdd.filter(lambda f: '-' not in str(f[columna]))\\\n",
    "                .map(lambda x: (1, float(x[columna])))\\\n",
    "                .reduceByKey(lambda x, y: max(x,y)).collect()[0][1]\n",
    "            \n",
    "            RDD = RDD.withColumn(columna, (RDD[columna] - valor_min)/ (valor_max - valor_min))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proves Denial constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = paths[1]\n",
    "RDD=sc.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:duckdb:{path}\") \\\n",
    "        .option(\"dbtable\", \"RDD\") \\\n",
    "        .option(\"driver\", \"org.duckdb.DuckDBDriver\").load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí aniria codis de barris únics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mapInArrow() missing 1 required positional argument: 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruthparajo/Desktop/uni/BDA/projecte final/FreshData-BDA/trusted_zone.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Denial Constraint: Unique Employee IDs\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m duplicate_ids \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mmapInArrow(\u001b[39mlambda\u001b[39;49;00m x: (x\u001b[39m.\u001b[39;49mselect(\u001b[39m'\u001b[39;49m\u001b[39mNom_Barri\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39mreduceByKey(\u001b[39mlambda\u001b[39;00m x, y: x \u001b[39m+\u001b[39m y)\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m duplicate_ids\u001b[39m.\u001b[39misEmpty():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mData quality rule satisfied: Unique Employee IDs\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: mapInArrow() missing 1 required positional argument: 'schema'"
     ]
    }
   ],
   "source": [
    "# Denial Constraint: Unique Employee IDs\n",
    "duplicate_ids = df.mapInArrow(lambda x: (x.select('Nom_Barri'), 1)).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > 1)\n",
    "if duplicate_ids.isEmpty():\n",
    "    print(\"Data quality rule satisfied: Unique Employee IDs\")\n",
    "else:\n",
    "    print(\"Data quality rule violated: Duplicate Employee IDs found\")\n",
    "    # Action to clean the data\n",
    "    # For example, remove duplicates or keep only one occurrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La nateja del average rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 01:37:46 ERROR Executor: Exception in task 0.0 in stage 65.0 (TID 57)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2364, in __getitem__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: 'Average _rent' is not in list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/lx/01q73krn1md9zqcwvy3rqlqh0000gn/T/ipykernel_2471/220209361.py\", line 1, in <lambda>\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2369, in __getitem__\n",
      "    raise PySparkValueError(item)\n",
      "pyspark.errors.exceptions.base.PySparkValueError: Average _rent\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/14 01:37:46 WARN TaskSetManager: Lost task 0.0 in stage 65.0 (TID 57) (192.168.28.41 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2364, in __getitem__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: 'Average _rent' is not in list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/lx/01q73krn1md9zqcwvy3rqlqh0000gn/T/ipykernel_2471/220209361.py\", line 1, in <lambda>\n",
      "  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2369, in __getitem__\n",
      "    raise PySparkValueError(item)\n",
      "pyspark.errors.exceptions.base.PySparkValueError: Average _rent\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/04/14 01:37:46 ERROR TaskSetManager: Task 0 in stage 65.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 57) (192.168.28.41 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2364, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'Average _rent' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/lx/01q73krn1md9zqcwvy3rqlqh0000gn/T/ipykernel_2471/220209361.py\", line 1, in <lambda>\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2369, in __getitem__\n    raise PySparkValueError(item)\npyspark.errors.exceptions.base.PySparkValueError: Average _rent\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2364, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'Average _rent' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/lx/01q73krn1md9zqcwvy3rqlqh0000gn/T/ipykernel_2471/220209361.py\", line 1, in <lambda>\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2369, in __getitem__\n    raise PySparkValueError(item)\npyspark.errors.exceptions.base.PySparkValueError: Average _rent\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruthparajo/Desktop/uni/BDA/projecte final/FreshData-BDA/trusted_zone.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m transformed_rdd \u001b[39m=\u001b[39m filtered_rdd\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mx, \u001b[39m\"\u001b[39m\u001b[39mAverage _rent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39meuro/month\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Show the resulting RDD\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ruthparajo/Desktop/uni/BDA/projecte%20final/FreshData-BDA/trusted_zone.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m transformed_rdd\u001b[39m.\u001b[39;49mcollect()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 57) (192.168.28.41 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2364, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'Average _rent' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/lx/01q73krn1md9zqcwvy3rqlqh0000gn/T/ipykernel_2471/220209361.py\", line 1, in <lambda>\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2369, in __getitem__\n    raise PySparkValueError(item)\npyspark.errors.exceptions.base.PySparkValueError: Average _rent\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2364, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'Average _rent' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/lx/01q73krn1md9zqcwvy3rqlqh0000gn/T/ipykernel_2471/220209361.py\", line 1, in <lambda>\n  File \"/Users/ruthparajo/Library/Python/3.9/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2369, in __getitem__\n    raise PySparkValueError(item)\npyspark.errors.exceptions.base.PySparkValueError: Average _rent\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "filtered_rdd = RDD.rdd.filter(lambda f: f[\"Average _rent\"] == 'average rent (euro/month)')\n",
    "\n",
    "# Replace values in the \"Average _rent\" column with \"euro/month\"\n",
    "transformed_rdd = filtered_rdd.map(lambda x: {**x, \"Average _rent\": \"euro/month\"})\n",
    "\n",
    "# Show the resulting RDD\n",
    "transformed_rdd.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
